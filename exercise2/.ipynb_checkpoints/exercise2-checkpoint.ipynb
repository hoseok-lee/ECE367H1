{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last name: Ho Seok\n",
    "### First name: Lee\n",
    "### Student number: 1004112177\n",
    "### List of collaborators (if any): \n",
    "* Damrongpiriyapong, Soraphol\n",
    "* Last name, first name\n",
    "* Last name, first name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise narrated and partially prepared by me (Mohannad Shehadeh), we'll be looking at some applications of eigenvalues and eigenvectors. Extremely useful and powerful tools can be developed once the theory of diagonalizability is studied, but for this exercise, we will only be concerned with a precursory emperical study of eigenvalues and eigenvectors. \n",
    "\n",
    "Before proceeding, I'd like to provide a reminder again for you to try to read *everything* written in this notebook carefully. Generally speaking, despite the fact that I like to ramble, everything that's written in the notebook is directly relevant to you being able to do some part of the exercise correctly and *easily.*\n",
    "\n",
    "We'll start by reviewing some basic facts and clarifying some common sources of confusion about eigenvalues and eigenvectors. A very elementary understanding of these facts will suffice for the purposes of this exercise.\n",
    "- A **nonzero** vector $\\mathbf x$ is an eigenvector of a square matrix $\\mathbf A$ if $\\mathbf A \\mathbf x = \\lambda \\mathbf x$ for some scalar $\\lambda$ (which could be zero) \n",
    "- $\\lambda$ is called an eigenvalue of $\\mathbf A$ and $\\mathbf x$ is called a corresponding eigenvector\n",
    "- If $\\mathbf x$ is an eigenvector of $\\mathbf A$, then $c \\mathbf x$  is also an eigenvector of $\\mathbf A$ for any nonzero scalar $c$ \n",
    "- If $\\mathbf x$ and $\\mathbf y$ are linearly independent eigenvectors corresponding to the same eigenvalue $\\lambda$, then you can verify that any linear combination of them is also an eigenvector corresponding to that eigenvalue. This means that any nonzero vector in their span which is a two-dimensional subspace is also an eigenvector \n",
    "- For this reason, it is more precise to talk about an eigenspace corresponding to a particular eigenvalue\n",
    "- With this in mind, when we refer to \"the\" eigenvector corresponding to some $\\lambda$  where there is no other linearly independent eigenvector corresponding to $\\lambda$, we usually mean that it's some choice of normalization and sign of eigenvector (which would form a basis for the corresponding one-dimensional eigenspace, a line)\n",
    "- When we refer to an eigenvalue as having \"two\" eigenvectors, we usually mean that it has two linearly independent eigenvectors meaning that any nonzero linear combination of them is an eigenvector, and the \"two\" refers to any linearly independent pair (which would form a basis for the corresponding two-dimensional eigenspace)\n",
    "- In summary, if a matrix has an eigenvector, then it has infinitely many living in an associated eigenspace, and sometimes the term \"eigenvectors\" is used interchangeably to refer to basis vectors for that eigenspace, or all the nonzero vectors in the eigenspace itself which is a source of confusion\n",
    "- From these statements, it is clear that eigenvectors corresponding to the same eigenvalue may or may not be linearly independent. However, it can be shown that eigenvectors corresponding to distinct eigenvalues are always linearly independent. You can prove this by induction. \n",
    "\n",
    "\n",
    "If you've understood and retained all of these statements, especially the first and last, then you can be assured that you're already ahead of many people in your understanding of eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An aside on a Julia technicality: \n",
    "\n",
    "Obtaining the transpose of a matrix ```A``` in Julia using ```'``` (conjugate transpose) or ```transpose``` does not actually create a transposed version of the matrix to avoid wasting time and space (memory). Rather it creates an object which will behave like the transpose when you print it or multiply it by other matrices. To obtain an actual transpose, you can use ```copy``` as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LinearAlgebra\n",
    "using DelimitedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1 2 3; 4 5 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A' # Of type \"Adjoint\" (meaning complex conjugate of transpose or Hermitian transpose, same as transpose if real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy(A') # What you'd expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Eigenvalues and eigenvectors of geometric transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two-dimensional rotation matrix is given by:\n",
    "\\begin{equation}\n",
    "    R(\\theta) = \n",
    "    \\begin{bmatrix}\n",
    "    \\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "    \\sin(\\theta) & \\cos(\\theta)\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "A hyperbolic rotation matrix is given by:\n",
    "\\begin{equation}\n",
    "    H(\\phi) = \n",
    "    \\begin{bmatrix}\n",
    "    \\cosh(\\phi) & \\sinh(\\phi) \\\\\n",
    "    \\sinh(\\phi) & \\cosh(\\phi)\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R(θ) = [cos(θ) -sin(θ); sin(θ) cos(θ)];\n",
    "H(ϕ) = [cosh(ϕ) sinh(ϕ); sinh(ϕ) cosh(ϕ)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that the eigenvalues of $H(\\phi)$ are $\\lambda_1 = e^\\phi$ and $\\lambda_2 = e^{-\\phi}$ with corresponding eigenvectors \n",
    "\n",
    "$$\n",
    "u_1 = \\begin{bmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\quad \n",
    "u_2 = \\begin{bmatrix}\n",
    "1\\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute these using the ```eigen``` function as follows for $\\phi = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = eigen(H(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig.values # Vector of eigenvalues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig.vectors # Matrix whose columns are the corresponding eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that these correspond to different constant multiples (possibly negative) of what we claimed were \"the\" eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If** the eigenvalues are real, they can be sorted. \n",
    "\n",
    "However, the default sorting behavior is currently inconsistent across versions of Julia for a technical reason so don't assume that they'll be sorted in any way. Or, double check manually that they are sorted the way you want them to be if you write code which relies on them being sorted in some way. \n",
    "\n",
    "Alternatively, we can, for example, find the eigenvector corresponding to the largest eigenvalue as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax(eig.values) # Gets index of largest eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig.vectors[:,argmax(eig.values)] # Gets the vector in the corresponding column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot on the left a collection of vectors obtained by rotating $\\begin{bmatrix}1 & 0\\end{bmatrix}^T$ and on the right, the result of applying $H(\\phi)$ to those vectors. You should experiment with changing the number of vectors $N$ and the parameter $\\phi$, but the choices of $N = 8,\\, \\phi = 0.82$ are good for answering the question that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "ϕ = 0.82\n",
    "p1 = quiver([0],[0],quiver=([0],[0]),xlim=(-2,2), ylim=(-2,2), aspect_ratio = :equal)\n",
    "for k = 1:N \n",
    "    θ = 2*pi*k/N\n",
    "    u = R(θ) * [1; 0]\n",
    "    quiver!([0],[0],quiver=([u[1]],[u[2]]))\n",
    "end\n",
    "p2 = quiver([0],[0],quiver=([0],[0]),xlim=(-2,2), ylim=(-2,2), aspect_ratio = :equal)\n",
    "for k = 1:N\n",
    "    θ = 2*pi*k/N\n",
    "    u = R(θ) * [1; 0]\n",
    "    v = H(ϕ) * u \n",
    "    quiver!([0],[0],quiver=([v[1]],[v[2]]))\n",
    "end \n",
    "plot(p1,p2,layout=(1,2),legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How does the above plot illustrate what you know about the two eigenvalues and eigenvectors of $H(\\phi)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two-dimensional horizontal shear matrix is given by:\n",
    "\\begin{equation}\n",
    "    S(\\beta) = \n",
    "    \\begin{bmatrix}\n",
    "    1 & \\beta \\\\\n",
    "    0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S(β) = [1 β; 0 1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that $S(\\beta)$ has eigenvalue $\\lambda = 1$ and eigenvector\n",
    "$$\n",
    "u = \\begin{bmatrix}\n",
    "1\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute these using the ```eigen``` function for $\\beta = 4$, though they are independent of $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen(S(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's really only \"one\" eigenvector here, but this doesn't mean that you can't have different (linearly independent) eigenvectors corresponding to the same eigenvalue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot on the left a collection of vectors obtained by rotating $\\begin{bmatrix}1 & 0\\end{bmatrix}^T$ and on the right, the result of applying $S(\\beta)$ to those vectors. You should experiment with changing the number of vectors $N$ and the parameter $\\beta$, but the choices of $N = 8,\\, \\beta = 1.5$ are good for answering the question that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "β = 1.5\n",
    "p1 = quiver([0],[0],quiver=([0],[0]),xlim=(-2,2), ylim=(-2,2), aspect_ratio = :equal)\n",
    "for k = 1:N\n",
    "    θ = 2*pi*k/N\n",
    "    u = R(θ) * [1; 0]\n",
    "    quiver!([0],[0],quiver=([u[1]],[u[2]]))\n",
    "end \n",
    "p2 = quiver([0],[0],quiver=([0],[0]),xlim=(-2,2), ylim=(-2,2), aspect_ratio = :equal)\n",
    "for k = 1:N\n",
    "    θ = 2*pi*k/N\n",
    "    u = R(θ) * [1; 0]\n",
    "    v = S(β) * u \n",
    "    quiver!([0],[0],quiver=([v[1]],[v[2]]))\n",
    "end \n",
    "plot(p1,p2,layout=(1,2),legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How does the above plot illustrate what you know about the eigenvalue and eigenvector of $S(\\beta)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rayleigh quotient with respect to some matrix $\\mathbf A$ is defined as \n",
    "$$\n",
    "R(\\mathbf v)=\\frac{\\mathbf{v^T}A\\mathbf v}{\\mathbf{v^T}\\mathbf{v}}\n",
    "$$\n",
    "with $\\mathbf v \\neq \\mathbf 0$. For symmetric matrices like $H(\\phi)$, the Rayleigh quotient has some notable properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ(A,v) = v'*A*v/(v'*v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot the Rayleigh quotient for $H(1)$ providing a contour plot on the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen(H(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [c for c in -1:0.05:1]\n",
    "y = [c for c in -1:0.05:1]\n",
    "RQ_H(x,y) = RQ(H(1), [x, y])\n",
    "p1 = plot(x, y, RQ_H, st =:surface,legend=:none)\n",
    "p2 = plot(x, y, RQ_H, st =:contourf,aspect_ratio = :equal)\n",
    "plot(p1,p2,layout=(1,2),legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** From examining the plot, what are the maxima and minima of the Rayleigh quotient and where do they occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll be providing some examples of simple Markov chains as a precursor to the final part. You are not expected to understand Markov chains deeply here as it is beyond the scope of the course. Understanding what we are trying to compute is enough and some amount of explanation is provided to give you some intuition for what it means. \n",
    "\n",
    "The following is an example of a simple Markov chain taken from https://en.wikipedia.org/wiki/Examples_of_Markov_chains\n",
    "![](Markov_Chain_weather_model_matrix_as_a_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertices of this graph (Sunny, Rainy) are called states and edges (arrows) represent probabilities of transitioning between states at any given time. We can represent this as a matrix $\\mathbf P$ where entry $i,j$ is the probability of transitioning to state $j$ from state $i$ at any given time. We get\n",
    "$$\n",
    "\\mathbf P = \\begin{bmatrix}\n",
    "0.9 & 0.1 \\\\\n",
    "0.5 & 0.5 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We can let a vector $\\mathbf x_k$ contain the probabilities of a sunny or rainy day at day $k$. If it's sunny today, we will have $\\mathbf x_0 = \\begin{bmatrix} 1 & 0 \\end{bmatrix}$.\n",
    "The probabilities of a sunny or rainy day tomorrow can be obtained as \n",
    "$$\n",
    "\\mathbf x_1 = \\mathbf x_0 \\mathbf P = \\begin{bmatrix} 1 & 0 \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.9 & 0.1 \\\\\n",
    "0.5 & 0.5 \\\\\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix} 0.9 & 0.1 \\end{bmatrix}\n",
    "$$\n",
    "The probability of a sunny or rainy day the day after tomorrow can be obtained as\n",
    "$$\n",
    "\\mathbf x_2 = \\mathbf x_1 \\mathbf P = \\mathbf x_0 \\mathbf P^2\n",
    "= \\begin{bmatrix} 0.86 & 0.14 \\end{bmatrix}\n",
    "$$\n",
    "And in general, the probability of a sunny or rainy day at day $n$ is given by\n",
    "$$\n",
    "\\mathbf x_n = \\mathbf x_{n-1} \\mathbf P = \\mathbf x_0 \\mathbf P^n\n",
    "$$\n",
    "Note that the rows of the matrix always sum up to one, as well as the entries of $\\mathbf x_k$ since they represent probabilities. Equivalently, the sums of the numbers on the outgoing edges of a vertex of the graph add up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [0.9 0.1; 0.5 0.5] # Transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(P, dims=2) # Compute the sum along the rows, across dimension 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = [1 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = x_0*P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = x_1*P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = x_0*P^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that as we raise the matrix to higher powers, it appears to converge to a matrix with constant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P^5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P^10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P^100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that for Markov chains with certain properties, the powers of the transition matrix will indeed converge to some matrix with constant columns. (Though this is not simple or obvious so don't worry about it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, left multiplying by our $\\mathbf x_0$ or any $\\mathbf x_k$ where the entries add up to 1 will just give us a row of the matrix. This is called the steady state distribution of the Markov chain and will represent the long-term probability of each state, i.e., the long-term proportions of sunny and rainy days. \n",
    "We consequently expect that \n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\mathbf x_n =  \\lim_{n \\to \\infty} \\mathbf x_0 \\mathbf P^n = \\lim_{n \\to \\infty} \\mathbf x_{n-1} \\mathbf P\n",
    "$$\n",
    "will converge to a steady state distribution $\\mathbf x_\\infty$. We would intuitively expect from the above relationships that $\\mathbf x_\\infty = \\mathbf x_\\infty \\mathbf P$, i.e., that $\\mathbf x_\\infty$ is a left eigenvector of $\\mathbf P$. This is equivalent to $\\mathbf{x_\\infty^T} = \\mathbf{P^T}\\mathbf{x_\\infty^T}$. So we just need to find an eigenvector of $\\mathbf{P^T}$ corresponding to eigenvalue $1$ and normalized so that the entries add up to 1. It can be shown that this is indeed what the rows of $\\mathbf P^n$ converge to. Moreover, it can also be shown that $1$ is the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that this is indeed the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = eigen(copy(P'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to get the eigenvector corresponding to eigenvalue $1$ which is the largest eigenvalue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inf = eig.vectors[:,argmax(eig.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inf = x_inf./sum(x_inf) # Normalize so entries add up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert norm(x_inf - (P^100)[1,:]) < 10^-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a Markov chain taken from https://en.wikipedia.org/wiki/Markov_chain\n",
    "![](Finance_Markov_chain_example_state_space.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Taking Bull market as the first state, Bear market as the second, and Stagnant market as the third, compute the steady state distribution of this markov chain $\\mathbf x_\\infty$ and store the result in a variable called ```x_inf```. (You can use the ```eigen``` function as we just did.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer should match the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inf_ref = [0.625, 0.3125, 0.0625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert norm(x_inf - x_inf_ref) < 10^-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Numerical methods for computing eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will implement three numerical methods for computing eigenvectors. Before starting, read Example 3.5 on pages 73-75 of your textbook. You will mainly need this for Part 4 but it provides context for this part. Following this, read through pages 199-204 of your textbook. It won't be necessary to understand everything fully since you will only need to implement the three algorithms described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the three algorithms for computing eigenvectors described on pages 199-204 of your textbook and reproduce the plot in Figure 7.1.\n",
    "\n",
    "This part will be graded solely on whether your plot resembles the plot of the Figure 7.1. Specifically, you will get 1 mark per curve of similar shape starting and ending in a similar place to the plot in the figure. Be sure to include a legend in your plot. Any syntax you might need is present in numerical exercises 0 and 1, but you can use whatever syntax you'd like.\n",
    "\n",
    "You will need to do this as described in the textbook to get a similar plot, i.e., using $\\sigma = 0.9$ for Algorithm 2, and $\\sigma = 0.9$ for the first two iterations of Algorithm 3 followed by $\\sigma$ computed by Rayleigh quotient for the remaining 18 iterations. The other implementation details, you can determine by yourself as long as you produce a similar plot to the one in the textbook at the end. If you use a random initialization, you might need to re-run your algorithm a few times to get a good result. \n",
    "\n",
    "A remark on Algorithm 3: For Algorithm 3, the matrix to be inverted in line 4 becomes singular upon convergence which happens quickly. You might want to terminate the algorithm once it's singular and replicate the last output of the algorithm for the purposes of producing a plot with 20 iterations. Alternatively, you can add a conditional statement to skip line 4 once the matrix is singular effectively freezing the output of the algorithm for the remaining iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Example 3.5 on pages 73-75 of your textbook if you haven't already. You have been provided with two files ```pagerank_urls.txt``` and ```pagerank_adj.csv```. \n",
    "\n",
    "The first file contains a list of 2571 URLs  of webpages which we'll associate with the numbers 1 to 2571.\n",
    "\n",
    "The provided data files consist of a modified version of web crawling data downloaded from https://www.limfinity.com/ir. The original dataset was created in January, 2004 so you might notice that some of the links are inactive. \n",
    "\n",
    "The second file contains a 2571 by 2571 matrix $\\mathbf J$ where entry $i,j$ is $1$ if there exists a link from webpage $j$ to webpage $i$ and $0$ otherwise. The data has been filtered to not allow for links from a webpage to itself and not allow webpages without outgoing links.\n",
    "\n",
    "The link matrix $\\mathbf A$ described in the textbook can be obtained as \n",
    "\n",
    "$$\n",
    "\\mathbf A_{i,j} = \\frac{\\mathbf J_{i,j}}{\\sum_{k=1}^N \\mathbf J_{k,j}}\n",
    "$$\n",
    "\n",
    "(with $N = 2571$).\n",
    "\n",
    "The columns of this link matrix $\\mathbf A$ should add up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = readdlm(\"pagerank_urls.txt\", ',', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = readdlm(\"pagerank_adj.csv\", ',', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = size(J)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using your implementation of Algorithm 1 (power iteration method) with as many iterations as you need, obtain the page score vector $\\mathbf x$ with entries adding up to 1 from the **modified link matrix** described in your textbook (with $\\mu = 0.15$). Store the vector in a variable called ```x```.\n",
    "\n",
    "A tip:\n",
    "- **If** you'd like to compare to the value of the eigenvector produced by the ```eigen``` function, note that the eigenvalues and eigenvectors produced may be complex due to numerical precision error, but the relevant ones will have tiny imaginary part so you can obtain the real part using the ```real``` function. Moreover, remember to normalize your eigenvectors so that the entries are nonnegative and add up to 1 before comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will print the 7 most important and least important webpages ranking by the value of the corresponding component in the eigenvector ```x```. The top 7 indices should be 2, 35, 36, 58, 49, 41, and 25. If those are what you get, then you've computed the eigenvector correctly. The indices of the worst seven webpages may vary by implementation due numerical precision error since their scores are very close to $0$. However, they should be similar kinds of URLs across implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sortperm(x, rev=true)[1:7]\n",
    "    println(\"Index: $(i)  Score: $(round(x[i],digits=4)) URL: $(URLs[i])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sortperm(x)[1:7]\n",
    "    println(\"Index: $(i)  Score: $(round(x[i],digits=4)) URL: $(URLs[i])\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Reading the URLs above, do the rankings make sense? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** If the link matrix $\\mathbf A$ is transposed, it takes on the same structure as the transition matrices for the Markov chains described earlier hence describing a Markov chain in the same way as described earlier. How can the PageRank algorithm be interpreted in terms of Markov chains?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
