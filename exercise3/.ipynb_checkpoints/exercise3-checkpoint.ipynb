{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last name: Lee\n",
    "### First name: Ho Seok\n",
    "### Student number: 1004112177\n",
    "### List of collaborators (if any): \n",
    "* Damrongpiriyapong, Soraphol\n",
    "* Last name, first name\n",
    "* Last name, first name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise prepared by me (Mohannad Shehadeh), we'll be empirically studying a technique often referred to as principal component analysis (PCA) and some related things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```using Pkg``` followed by ```Pkg.add()``` as used in Numerical Exercise 0 to install any of the following packages if any of them is not installed already. (If the next cell runs without error, they're installed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LinearAlgebra\n",
    "using DelimitedFiles\n",
    "using Statistics\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: A simple exercise and some preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper and lower-triangular systems of equations can be solved by very simple algorithms described on page 207 of your textbook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly generate a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1 Array{Float64,2}:\n",
       " 0.9342007416975482 \n",
       " 0.9878214397351275 \n",
       " 0.5483828548034806 \n",
       " 0.09058200997758159\n",
       " 0.7034832020900119 \n",
       " 0.17387927494547806\n",
       " 0.2513701051808097 \n",
       " 0.417632228873845  \n",
       " 0.9332531447348988 \n",
       " 0.3667687930843655 "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ref = rand(10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly generate a lower-triangular matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Array{Float64,2}:\n",
       " 0.653177  0.0        0.0        0.0       …  0.0       0.0        0.0     \n",
       " 0.759207  0.640847   0.0        0.0          0.0       0.0        0.0     \n",
       " 0.114528  0.824524   0.0934658  0.0          0.0       0.0        0.0     \n",
       " 0.156846  0.101342   0.665449   0.689852     0.0       0.0        0.0     \n",
       " 0.759822  0.422276   0.347225   0.821927     0.0       0.0        0.0     \n",
       " 0.497078  0.742904   0.320823   0.21009   …  0.0       0.0        0.0     \n",
       " 0.889325  0.0486173  0.887431   0.379506     0.0       0.0        0.0     \n",
       " 0.499709  0.429963   0.845449   0.569808     0.132389  0.0        0.0     \n",
       " 0.163857  0.759735   0.207564   0.110313     0.61485   0.0383611  0.0     \n",
       " 0.166958  0.0726243  0.527134   0.580726     0.465016  0.509182   0.844303"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [rand()*(j <= i) for i in 1:10, j in 1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be invertible with high probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = A*x_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the forward substitution algorithm described on page 207 of your textbook and use it to solve the system $y = A x$ for $x$ with the $A$ and $y$ being the ```A``` and ```y``` just randomly defined. Store your answer in a variable called ```x``` which you can compare to the correct answer ```x_ref```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(x-x_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert norm(x-x_ref) < 10^-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now discuss some properties of a certain kind of Hermitian matrix. Some, you might have seen in class, others, not yet. Regardless, we will only investigate these empirically in this exercise so don't fret! \n",
    "\n",
    "- A Hermitian matrix $A$ is a square matrix satisfying $A = A^H$ where $H$ denotes the Hermitian transpose. If the entries of the matrix are real, then $A^H = A^T$ so any real symmetric matrix, one where $A = A^T$, is also Hermitian. \n",
    "- In other words, Hermitians and transposes are the same thing when working with real matrices, and anything I claim to be true for Hermitian matrices is automatically true for real symmetric matrices because every real symmetric matrix is Hermitian\n",
    "- As with the transpose, $(A^H)^H = A$ and $(AB)^H = B^HA^H$\n",
    "- Observe that matrices of the form $A = BB^H$ are Hermitian because $A^H = (BB^H)^H = (B^H)^H B^H = BB^H = A$. The same thing applies to matrices of the form $A = B^HB$\n",
    "- $B$ can be a wide or tall rectangular matrix \n",
    "- Hermitian matrices have some very strong properties, and Hermitian matrices that can be written as $BB^H$ have some even stronger properties\n",
    "- We will illustrate just some of these properties by examples and without proofs, but you will learn why these are true later in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a Hermitian matrix of the form $BB^H$. Note that ```im``` denotes the imaginary unit in Julia and recall that ```'``` yields the Hermitian transpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = rand(3,3) + im*rand(3,3)\n",
    "A = B*B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the diagonal elements of the matrix above must be real since they must be equal to their own complex conjugates because diagonal elements are not changed by transposing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A-A' # Zeros because A == A' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain its eigenvalues and eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = eigen(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the eigenvalues are real. This turns out to be true for any Hermitian matrix, so we can always talk about ordering the eigenvalues of a Hermitian matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = eig.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the eigenvectors corresponding to distinct eigenvalues are orthogonal! Since this matrix has three distinct eigenvalues, all the eigenvectors are pairwise orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W'*W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you can't see all the entries above due to the space on your screen, let's do a quick little check by summing all the entries of the matrix and this should be just the sum of the diagonal entries of an identity matrix since the other components are zero (or very close to zero):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(W'*W) # Should be three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Hermitian matrix need not have all the eigenvalues be distinct, but for this one which we randomly generated, it's highly unlikely to have repeated eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate one where we force it to have some repeated eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = W*[1 0 0; 0 1 0; 0 0 2]*W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = eigen(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = eig.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W*W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(W*W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors are still orthogonal! But this doesn't mean that any two linearly independent eigenvectors corresponding to the eigenvalue $1$ are orthogonal. What it means is that the eigenspace corresponding to the eigenvalue $1$ is two-dimensional and we can find a pair of orthonormal vectors corresponding to the eigenvalue $1$ which is what Julia chooses to give us. \n",
    "\n",
    "This last fact is not very important for this exercise in particular so don't worry if it's not clear because it will be covered in class soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now discuss an important class of Hermitian matrices that can be written as $BB^H$, covariance matrices. \n",
    "\n",
    "What I will describe has equivalent interpretations and descriptions in the language of probability, in the language of statistics, and in the language of optimization, but I will avoid such language so that it's understandable to everyone. I will ocassionally mention technicalities in case you have studied such topics or will study them at some point in the future, but you can safely ignore these technicalities if you're just interested in completing the exercise. \n",
    "\n",
    "Suppose we had some collection of $N$ $M$-dimensional vectors $x \\in R^{M\\times1}$ each representing an example or observation of a particular kind of data. Each $M$-dimensional vector could represent, for example:\n",
    "- Time-series data where the components of the vector represent a quantity evolving in time like stock price data, Google Trends popularity data, or an hourly temperature forecast, etc.\n",
    "- A collection of measurements from an experiment. For example, each component of the vector could represent a different physical quantity like the density, mass, and volume of a rock. The $N$ samples would then represent the densities, masses, and volumes of $N$ different observed rocks\n",
    "\n",
    "Let this collection of vectors $x^{(j)} \\in R^{M\\times 1}$ for $j = 1,2,\\dots,N$ be the columns of some matrix $X \\in R^{M\\times N}$\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x^{(1)} & x^{(2)} & \\dots & x^{(N)}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Let $\\bar{x} \\in R^{M\\times 1}$ be the average vector across all samples\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{N}\\sum_{j = 1}^N x^{(j)}.\n",
    "$$\n",
    "Equivalently, the components of $\\bar x$ are \n",
    "$$\n",
    "\\bar{x}_i = \\frac{1}{N}\\sum_{j = 1}^N x^{(j)}_i.\n",
    "$$\n",
    "The sample covariance matrix is the $M \\times M$ matrix $Q$ whose entries are given by \n",
    "$$\n",
    "Q_{ij} = \\frac{1}{N-1} \\sum_{k=1}^N(x_i^{(k)} - \\bar{x}_i)(x_j^{(k)} - \\bar{x}_j).\n",
    "$$\n",
    "We can always presubtract the mean vector from all of our samples so that we get a new dataset with $\\bar x = \\mathbf 0$ and we can add it back if we need it so we can assume that $\\bar x = \\mathbf 0$ without loss of generality. In this case, $Q$ is simply given by \n",
    "$$\n",
    "Q = \\frac{1}{N-1} XX^T\n",
    "$$\n",
    "\n",
    "$Q_{ij}$ is the sample correlation of the $i$th and $j$th component of this vector data. \n",
    "\n",
    "We will be interested in the eigenvectors (or eigenspaces) of this matrix in this exercise and the constant term $\\frac{1}{N-1}$ does not affect them. It only scales the eigenvalues by the same constant, so you can actually ignore it for this exercise and let \n",
    "$$\n",
    "Q = XX^T.\n",
    "$$\n",
    "\n",
    "I will refer to $Q$ as the covariance matrix of the data in this exercise, however, this is **not** what a **covariance matrix** is. Strictly speaking $Q$ is equal to $N-1$ times an unbiased estimator of the covariance matrix of a random vector, but the meaning of this and the distinction is irrelevant to the exercise.  \n",
    "\n",
    "All you need to know for this exercise is that $Q_{ij}$ is a number proportional to the correlation of  $i$th and $j$th component of the vector data under consideration, and I just mention this detail so I don't mislead you into using wrong terminology if you study this in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Inferring a basis from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following collection of 100 randomly generated two-dimensional data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in 0.02:0.02:2]\n",
    "a = 0.8\n",
    "b = 0.2\n",
    "y = a.*x .+ b .+ 0.1*randn(100)\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect them in a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x'; y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the mean vector as described earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mean(X, dims = 2) # Computes the mean across the columns (along the rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace the data by a version with the mean subtracted.\n",
    "\n",
    "The following operation, subtraction of a column vector from a matrix, is interpreted by Julia as subtraction of the column vector from each column of the matrix. Equivalently, it subtracts from the matrix a matrix where all the columns are this column vector replicated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X .- m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is now zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(X, dims = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X[1,:],X[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this data lives in a two-dimensional space, it's clearly lying very close to a line, so we may ask the question, what $K$-dimensional subspace ($K = 1$ in this case) can I project the data onto to give me the best $K$-dimensional approximation of the data in some sense?\n",
    "\n",
    "The answer turns out to be that this is the eigenspace corresponding to the $K$ largest eigenvalues of the covariance matrix. I.e., we project onto the the span of the eigenvectors corresponding to the $K$ largest eigenvalues. \n",
    "\n",
    "Equivalently, we can ask the question of which subspaces capture most of the variation in the data, and the answer turns out to be that they are the ones corresponding to the largest eigenvalues of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = eigen(X*X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = eig.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W*W'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the eigenvector corresponding to the largest eigenvalue and plot it alongside the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = eig.vectors[:,argmax(eig.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X[1,:],X[2,:])\n",
    "quiver!([0],[0],quiver=([u[1]],[u[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, this eigenspace is the line along which the data is most spread out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since eigenvectors corresponding to distinct eigenvalues are orthogonal, projection is easy, we can obtain a matrix $W$ whose columns are an orthonormal basis for the subspace we'd like to project onto. In this case, there's just one basis vector, everything done would work if we had more basis vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to compute the projection coefficients for all data points. Noting that\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "W^T x^{(1)} & W^T x^{(2)} & \\dots & W^T x^{(N)}\n",
    "\\end{bmatrix} = W^T X\n",
    "$$\n",
    "\n",
    "we compute them as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = W'*X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the same kind of argument, we obtain the projections of all data points as a matrix $V$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = W*A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the means back, both to the data, and to the projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X .+ m\n",
    "V = V .+ m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a crucial step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will plot the data and its one-dimensional approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X[1,:],X[2,:])\n",
    "scatter!(V[1,:],V[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now revisit our Taylor Swift popularity data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS = readdlm(\"TaylorSwift.csv\", ',', '\\n')\n",
    "y = TS[:,2]\n",
    "y = convert(Array{Float64,1}, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the data has been truncated a bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to obtain some basis for a $24$-dimensional approximation of the Taylor Swift data by analyzing similar popularity data from other artists and bands. Data vectors for the following 30 artists/bands were obtained from Google Trends:\n",
    "\n",
    "Katy Perry, Ariana Grande, Miley Cyrus, Beyoncé, Lady Gaga, Nicki Minaj, Drake, Adele, Sam Smith, Post Malone, Khalid,  Billie Eilish, Lizzo, Shawn Mendes, Lorde, Ed Sheeran, Maroon 5, Imagine Dragons, The Weeknd, Kendrick Lamar, Carly Rae Jepsen, Kelly Clarkson, Avril Lavigne, Bruno Mars, One Direction, Alicia Keys, Mariah Carey, Kanye West, Coldplay, and Fergie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Artists = readdlm(\"Artists.csv\", ',', '\\n')\n",
    "Artists = convert(Array{Float64,2}, Artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = copy(Artists)\n",
    "plot(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, 30 examples is not a large enough amount of data and I couldn't find a way to obtain a larger amount easily, but we can improvise!\n",
    "\n",
    "We know that the data is time-series data with bumps occuring at different points in time so similar data can be artificially created by horizontally translating the existing data. We apply random horizontal translations done in a circular way (i.e., such that horizontally shifting a bump far to the right causes it to come out of the left like Pac-Man). \n",
    "\n",
    "We do this five times appending this randomly shifted data to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = copy(Artists)\n",
    "for i in 1:5\n",
    "    X = [X circshift(X_, (rand(Int)%154,0))]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 180 vectors in our collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Obtain a 24-dimensional approximation of the Taylor Swift data ```y``` by projection onto the subspace spanned by the eigenvectors corresponding to the 24 largest eigenvalues of the covariance matrix of the data ```X```.\n",
    "\n",
    "You will simply need to do the following steps in this order:\n",
    "- Obtain the mean of the artist data vectors ```X``` and subtract it from this data\n",
    "- Also subtract this mean from the Taylor Swift vector ```y``` \n",
    "- Obtain the orthonormal basis for the subspace ```W``` \n",
    "- Project ```y``` onto this subspace to get ```y_hat```\n",
    "- Add the mean to ```y_hat``` to get the final ```y_hat```\n",
    "- Add the mean back to ```y``` to get back the original ```y```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot your projection of the Taylor Swift data ```y_hat``` (store it in a variable called ```y_hat``` if you haven't already) and the original data ```y``` and compute the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(y)\n",
    "plot!(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(y-y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Comment on how this compares to the basis you chose in Numerical Exercise 1. Is it better or worse? Why do you think this is the case? Note that your answer could be different than that of your peers because there was randomness involved in creating the data when we did random circular shifts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Classifying wheat seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data obtained from: https://archive.ics.uci.edu/ml/datasets/seeds# consists of the attributes of three types of wheat seeds: Kama, Rosa and Canadian, which we will assign the numbers 1, 2, and 3.\n",
    "\n",
    "The following code will pre-process the data for you to have zero mean (no need to care about zeroing the mean in this part) and comparable units for each of the attributes. Moreover, the code will randomly split the data into two collections, one called ```X_TRAINING``` and another called ```X_TESTING```. \n",
    "\n",
    "An important remark: Once you have your final answer for this part, **don't re-run this next cell** because there is randomness in the splitting of the data and hence eigenvectors and hence your answers so your answers could become wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = readdlm(\"seeds_dataset.txt\", '\\t', '\\n') # Load data\n",
    "X = zeros(7,210) # Data\n",
    "Y = zeros(1,210) # Labels\n",
    "X[1,:] = convert(Array{Float64,1}, Data[:,1]) # 7 different attributes\n",
    "X[2,:] = convert(Array{Float64,1}, Data[:,2]) \n",
    "X[3,:] = convert(Array{Float64,1}, Data[:,3]) \n",
    "X[4,:] = convert(Array{Float64,1}, Data[:,4]) \n",
    "X[5,:] = convert(Array{Float64,1}, Data[:,5]) \n",
    "X[6,:] = convert(Array{Float64,1}, Data[:,6]) \n",
    "X[7,:] = convert(Array{Float64,1}, Data[:,7]) \n",
    "Y = Data[:,8] # Labels\n",
    "I = randperm(210) # Random permutation of indices\n",
    "I_TRAINING = I[1:160] # Training subset\n",
    "I_TESTING = I[160:210] # Testing subset\n",
    "X_TRAINING = X[:,I_TRAINING]\n",
    "Y_TRAINING = Y[I_TRAINING]\n",
    "means = mean(X_TRAINING, dims=2)[:] # Means\n",
    "std_factor = [(1.0./std(X_TRAINING, dims=2)[i])*(i == j) for i = 1:7, j = 1:7] # 1 / (standard deviations)\n",
    "X_TRAINING = X_TRAINING .- means # Subtract means from training data\n",
    "X_TRAINING = std_factor*X_TRAINING # Standardize training data\n",
    "X_TESTING = X[:,I_TESTING] \n",
    "Y_TESTING = Y[I_TESTING]\n",
    "X_TESTING = X_TESTING .- means # Subtract means and standardize testing data\n",
    "X_TESTING = std_factor*X_TESTING;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of the following matrix contains numbers corresponding to the following attributes of a type of seed but they have been scaled and a constant vector subtracted so that they are zero mean so they aren't exactly in physically interpretable form. \n",
    "\n",
    "1. area A,\n",
    "2. perimeter P,\n",
    "3. compactness C = 4*pi*A/P^2,\n",
    "4. length of kernel,\n",
    "5. width of kernel,\n",
    "6. asymmetry coefficient\n",
    "7. length of kernel groove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding type of seed for each of the examples in the columns is given by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in ```X_TESTING``` and ```Y_TESTING``` is another collection containing the same kind of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Obtain the eigenvectors of the covariance matrix of ```X_TRAINING``` corresponding to the two **smallest** eigenvalues and obtain the projection coefficients resulting from projection of the vectors in ```X_TRAINING``` onto the corresponding two-dimensional subspace. Store the result in a variable called ```feats```. ```feats``` should be a 2 by 160 matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the projection coefficients corresponding to the different types of seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(feats[1,Y_TRAINING .== 1],feats[2,Y_TRAINING .== 1])\n",
    "scatter!(feats[1,Y_TRAINING .== 2],feats[2,Y_TRAINING .== 2])\n",
    "scatter!(feats[1,Y_TRAINING .== 3],feats[2,Y_TRAINING .== 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Obtain the eigenvectors of the covariance matrix of ```X_TRAINING``` corresponding to the two **largest** eigenvalues and obtain the projection coefficients resulting from projection of the vectors in ```X_TRAINING``` onto the corresponding two-dimensional subspace. Store the result in a variable called ```feats```. ```feats``` should be a 2 by 160 matrix. Store the two eigenvectors as the columns of a matrix called ```W``` as you will need this in the last part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the projection coefficients corresponding to the different types of seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(feats[1,Y_TRAINING .== 1],feats[2,Y_TRAINING .== 1])\n",
    "scatter!(feats[1,Y_TRAINING .== 2],feats[2,Y_TRAINING .== 2])\n",
    "scatter!(feats[1,Y_TRAINING .== 3],feats[2,Y_TRAINING .== 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Comment on the difference between the two and provide your interpretation of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "We would like to create a classifier that simply determines based on the projection coefficients of the seed data onto the two-dimensional subspace of the covariance matrix corresponding to the two largest eigenvalues, whether a seed is of type 3 or not. The prediction will simply be 1 if the seed is of type 3 and 0 if the seed is not of type 3, i.e., of types 1 or 2. \n",
    "\n",
    "We will create this classifier by simply trying to find a line that separates the type 3 seed from the other seeds. \n",
    "\n",
    "All you have to do for this exercise is play around with 3 parameters to obtain this line and run the following two cells until you get a classification accuracy of 85% or higher on the testing data. \n",
    "\n",
    "The first two parameters you can change are ```a``` and ```b``` which are just the slope and vertical shift of the line. And the third parameter is the ```>``` which you can choose as ```>``` or ```<``` and this corresponds to whether the classification is yes/1/(type 3) or no/0/(types 1 or 2) for data points above or below the line. \n",
    "\n",
    "Note that the answer for all three of these will change if you re-run the cell which loads and processes the data because it randomly partitions the data so don't re-run after you have a good answer. \n",
    "\n",
    "Note further that the classification accuracy is computed on a different set of data than the data being currently visualized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(feats[1,Y_TRAINING .== 1],feats[2,Y_TRAINING .== 1])\n",
    "scatter!(feats[1,Y_TRAINING .== 2],feats[2,Y_TRAINING .== 2])\n",
    "scatter!(feats[1,Y_TRAINING .== 3],feats[2,Y_TRAINING .== 3])\n",
    "\n",
    "x = [i for i = -3:0.1:3]\n",
    "a = 0\n",
    "b = 0\n",
    "y = a*x .+ b\n",
    "plot!(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = W'*X_TESTING # Obtain projection coefficients for test data\n",
    "\n",
    "predictions = [V[2,i] < a*V[1,i]+b ? 1 : 0 for i in 1:size(X_TESTING)[2]] \n",
    "\n",
    "truths = 1*(Y_TESTING .== 3)\n",
    "\n",
    "accuracy = sum(predictions .== truths)/length(predictions)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert accuracy > 85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surely you can't do this by hand for higher dimensional data since a two-dimensional subspace might not be enough to discriminate. And as you might have guessed, there is a way to find this line, or more generally, hyperplane which best separates the classes and that will be the topic of a future exercise. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
