{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last name: Lee\n",
    "### First name: Ho Seok\n",
    "### Student number: 1004112177\n",
    "### List of collaborators (if any): \n",
    "* Damrongpiriyapong, Soraphol\n",
    "* Last name, first name\n",
    "* Last name, first name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by installing the ```Convex``` and ```SCS``` packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  99.9 %0.0 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=====>                                   ]  10.1 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=======>                                 ]  15.1 %>                             ]  27.3 %>                       ]  41.0 %                  ]  53.4 %3 %      ]  84.6 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [====================================>    ]  87.6 %\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Parsers ──────── v0.3.8\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m BenchmarkTools ─ v0.4.3\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m MacroTools ───── v0.5.2\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m StaticArrays ─── v0.12.1\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m JuliaFormatter ─ v0.1.19\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Convex ───────── v0.12.6\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m AbstractTrees ── v0.2.1\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m HTTP ─────────── v0.8.7\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m BinaryProvider ─ v0.5.8\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m MathProgBase ─── v0.7.7\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m DataStructures ─ v0.17.5\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m PlotUtils ────── v0.6.1\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m GR ───────────── v0.42.0\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [f65535da]\u001b[39m\u001b[92m + Convex v0.12.6\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [1520ce14]\u001b[39m\u001b[92m + AbstractTrees v0.2.1\u001b[39m\n",
      " \u001b[90m [6e4b80f9]\u001b[39m\u001b[92m + BenchmarkTools v0.4.3\u001b[39m\n",
      " \u001b[90m [b99e7846]\u001b[39m\u001b[93m ↑ BinaryProvider v0.5.6 ⇒ v0.5.8\u001b[39m\n",
      " \u001b[90m [f65535da]\u001b[39m\u001b[92m + Convex v0.12.6\u001b[39m\n",
      " \u001b[90m [864edb3b]\u001b[39m\u001b[93m ↑ DataStructures v0.17.2 ⇒ v0.17.5\u001b[39m\n",
      " \u001b[90m [28b8d3ca]\u001b[39m\u001b[93m ↑ GR v0.41.0 ⇒ v0.42.0\u001b[39m\n",
      " \u001b[90m [cd3eb016]\u001b[39m\u001b[93m ↑ HTTP v0.8.6 ⇒ v0.8.7\u001b[39m\n",
      " \u001b[90m [98e50ef6]\u001b[39m\u001b[93m ↑ JuliaFormatter v0.1.16 ⇒ v0.1.19\u001b[39m\n",
      " \u001b[90m [1914dd2f]\u001b[39m\u001b[93m ↑ MacroTools v0.5.1 ⇒ v0.5.2\u001b[39m\n",
      " \u001b[90m [fdba3010]\u001b[39m\u001b[92m + MathProgBase v0.7.7\u001b[39m\n",
      " \u001b[90m [69de0a69]\u001b[39m\u001b[93m ↑ Parsers v0.3.7 ⇒ v0.3.8\u001b[39m\n",
      " \u001b[90m [995b91a9]\u001b[39m\u001b[93m ↑ PlotUtils v0.6.0 ⇒ v0.6.1\u001b[39m\n",
      " \u001b[90m [90137ffa]\u001b[39m\u001b[93m ↑ StaticArrays v0.11.0 ⇒ v0.12.1\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m GR → `~/.julia/packages/GR/f1Iqi/deps/build.log`\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Convex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m MathOptInterface ─ v0.9.7\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m SCS ────────────── v0.6.2\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [c946c3f1]\u001b[39m\u001b[92m + SCS v0.6.2\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [b8f27783]\u001b[39m\u001b[92m + MathOptInterface v0.9.7\u001b[39m\n",
      " \u001b[90m [c946c3f1]\u001b[39m\u001b[92m + SCS v0.6.2\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m SCS → `~/.julia/packages/SCS/Y9wx3/deps/build.log`\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"SCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Convex [f65535da-76fb-5f13-bab9-19810c17039a]\n",
      "└ @ Base loading.jl:1242\n",
      "┌ Info: Precompiling SCS [c946c3f1-0d1f-5ce8-9dea-7daa1f7e2d13]\n",
      "└ @ Base loading.jl:1242\n"
     ]
    }
   ],
   "source": [
    "using Convex\n",
    "using SCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/hoseoklee/.julia/compiled/v1.2/Plots/ld3vC.ji for Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "using LinearAlgebra\n",
    "using DelimitedFiles\n",
    "using Statistics\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise narrated and partially prepared by me (Mohannad Shehadeh), we'll be learning how to use the ```Convex``` modelling language to solve convex optimization problems. Similar enormously popular frameworks exist for other languages like ```CVX``` for MATLAB and ```CVXPY``` for Python, but all of these (including ```Convex```) came out of the same research group and are based on the same principles so what you learn will be transferable should you need to use another language in your future work or research. Documentation for ```Convex``` is available at https://www.juliaopt.org/Convex.jl/dev/ and you should keep this at hand.\n",
    "\n",
    "For the purposes of this exercise, ```Convex``` will be acting as a magical blackbox which solves optimization problems and I will attempt to give you a hopefully not too hazy big picture view of how this works. This software has been designed for use by non-experts in convex optimization so a big picture view will suffice for doing a lot. In fact, learning to use this framework will give you a wizard-like ability to do things that would have required far more expertise and effort just 10 years ago and have endless and tremendous applications in industry and research.  \n",
    "\n",
    "In the remaining weeks of the course, you will explore a little bit more deeply some of the things which I will gloss over.\n",
    "\n",
    "Finally, as with previous exercises, reading and understanding the examples and instructions is the most important part and if you put effort there, only a small amount of simple code needs to be written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Background and simple exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start by providing the background needed to understand what the ```Convex``` package is and how it's used.  \n",
    "\n",
    "Note that slightly more general definitions might exist than the definitions I'll provide here, for example, generalizing from a vector space to an affine space. So, be careful for possible differences in definitions when studying the course material later.\n",
    "\n",
    "Let $x_1, \\dots, x_K$ be a set of points (vectors) in a vector space $R^N$. A *convex combination* of these points is a point of the form $\\alpha_1 x_1 + \\dots + \\alpha_K x_K$ where $\\alpha_i \\geq 0$ and $\\sum_{i = 1}^K \\alpha_i = 1$. In other words, a convex combination is a linear combination where the coefficients are nonnegative and add up to $1$.\n",
    "\n",
    "A *convex set* is a subset of $R^N$ that is closed under convex combinations. Refer to https://en.wikipedia.org/wiki/Convex_set for some pictures.\n",
    "\n",
    "Let $X \\subseteq R^N$ be a convex set and let be $f\\colon X \\to R$ be a function. $f$ is said to be a *convex function* if, for all $x,y \\in X$ and for any $\\lambda \\in R$ satisfying $0 \\leq \\lambda \\leq 1$\n",
    "\\begin{equation}\n",
    "    f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda) f(y)\n",
    "\\end{equation}\n",
    "\n",
    "Refer to https://en.wikipedia.org/wiki/Convex_function for some pictures.\n",
    "\n",
    "$f$ is said to be a *concave function* if $-f$ is a convex function, or, equivalently, the above inequality is reversed.\n",
    "\n",
    "An *affine function* is one which is simultaneously concave and convex in which case it must be of the form $h(x) = Ax + b$ for some $A \\in R^{M\\times N}$ and $b \\in R^M$.\n",
    "\n",
    "Convex functions, apart from occurring naturally in many applications, have convenient mathematical properties like that, for example, any local minimum of a convex function is also a global minimum. \n",
    "\n",
    "A *convex optimization problem* is an optimization problem, which, in the standard form, is  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& p^* = \\underset{x}{\\text{min}} & & f_0(x) \\\\\n",
    "& \\text{subject to} & &  f_i(x) \\leq 0,\\; \\text{for}\\; i = 1,\\dots,m \\\\\n",
    "& & & h_i(x) = 0,\\; \\text{for}\\; i = 1,\\dots,q\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "(with $x \\in R^N$) where:\n",
    "- the objective function $f_0$ is convex\n",
    "- the inequality constraint functions $f_i$ for $i = 1,\\dots,m$ are convex \n",
    "- the equality constraint functions $h_i$ for $i = 1,\\dots,q$ are affine\n",
    "\n",
    "It can be shown that the set of points satisfying such constraints, called the *feasible set* (or region), is a convex set $X$.\n",
    "\n",
    "If $X = R^N$, the problem is said to be *unconstrained*.\n",
    "\n",
    "Nonconvex optimization is extremely difficult in general (for humans and computers), hence our interest in problems which can be cast in this form. Luckily, it turns out that a huge variety of problems can indeed be naturally expressed in this form. \n",
    "\n",
    "However, efficient generic methods for solving arbitary convex optimization problems are still an active area of research, so we must restrict ourself a little bit further to a subcategory of convex optimization problems. Luckily for us, this subcategory of problems will still be fairly broad and you're unlikely to come up with a convex optimization problem that doesn't fit into it.\n",
    "\n",
    "A convex optimization problem where all of the functions involved, objectives and constraints, are linear or affine is called a linear program (LP). When the objective function is quadratic, it's called a quadratic program (QP). When the objective function and constraints are quadratic, it's called a quadratically constrained quadratic program (QCQP). Further classes include semidefinite programs (SDPs) and second-order cone programs (SOCPs). All of these categories of convex optimization problems fall into a broader category of convex optimization problems called cone programs (CPs). \n",
    "\n",
    "Cone programs can be solved highly efficiently by computers using cone program solvers. The ```SCS``` you installed was a cone program solver. Given a cone program, an appropriate specialized algorithm can be run to get to an optimal solution.\n",
    "\n",
    "It turns out that a wide variety of convex optimization problems can be converted into equivalent cone programs. So, once you have a convex optimization problem, traditionally, you would try to convert it into an equivalent LP, QP, QCQP, or other CP. Once your problem is in such a form, you can use the appropriate specialized solver to get to your solution.\n",
    "\n",
    "However, recognizing that a problem is convex, and figuring out how to convert it into a cone program can be quite difficult in general. You will learn how to do this in some common special cases in this course. \n",
    "\n",
    "Fortunately for you, the ```Convex``` modelling language (as well as ```CVX``` and ```CVXPY```) is exactly made for solving this problem. An approach called disciplined convex programming (DCP) is used to allow you to express a convex optimization problem in a way which allows ```Convex``` both to check that the problem is indeed convex and more importantly, convert the problem into a standard cone program form so that a specialized solver like ```SCS``` can be used to solve it. \n",
    "\n",
    "We will now demonstrate how DCP works by an example followed by some exercises. The overview is as follows:\n",
    "\n",
    "- We start by specifying **variables** which are the values we would like to optimize with respect to. We can have the variables be a single vector, multiple vectors, or multiple scalars or combinations of both. In any case, they can be interpreted as one big vector but you can specify them however you want.\n",
    "- We then specify **constants** which are numbers, vectors, and matrices involved in our objective and constraint functions which are fixed.\n",
    "- Next, we create our **objective** and **constraints** using **expressions**\n",
    "- These mathematical expressions must be built as a composition of predefined convex or affine functions called **atoms**. A list of these **atoms** can be found at https://www.juliaopt.org/Convex.jl/dev/operations/#Operations-1 and you must use them to create the expressions for your functions\n",
    "- You cannot combine these atoms in an arbitary way, you must do it in a way which preserves convexity, e.g., sums of convex functions are convex, a composition of a convex and increasing function with a convex function is convex, and more\n",
    "- ```Convex``` will check whether the expression you created follows the rules and will throw an error if it didn't so you won't actually have to worry about it, however, if you'd like to learn these rules, you can look at https://dcp.stanford.edu/rules\n",
    "- Finally, you define a **problem** using your objective and constraints which are made of expressions which are made of variables and ```Convex``` magically solves it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a remark on notation: Let $x,y \\in R^N$. The notation $x \\leq y$ will mean component-wise inequality, i.e., $x_i \\leq y_i$ for $i = 1,\\dots,N$.\n",
    "\n",
    "Consider the following linear program (adapted from [here](https://www.juliaopt.org/Convex.jl/dev/examples/general_examples/basic_usage/)) where $c, x \\in R^N$, $b \\in R^M$ and $A \\in R^{M \\times N}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& p^* = \\underset{x}{\\text{min}} & & c^Tx \\\\\n",
    "& \\text{subject to} & &  Ax \\leq b \\\\\n",
    "& & & x \\geq 1\\\\\n",
    "& & & x \\leq 10\\\\\n",
    "& & & x_2 \\leq 5\\\\\n",
    "& & & x_1 + x_4 - x_2 \\leq 10\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll define the constants of the problem. Let $N = M = 4$ and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1 0 0 0; 0 1 0 0; 0 0 1 0; 0 0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [10, 10, 10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the variables now. In this case, there's only one $4$-dimensional variable $x$. We define a variable using\n",
    "\n",
    "```<variable name> = Variable(<size>)``` \n",
    "\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to define our objective function. We are only allowed to create expressions from operations and atoms listed here: https://www.juliaopt.org/Convex.jl/dev/operations/#Operations-1. If you get an error when trying to create an expression, try to find what you need in that list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = c'*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add constraints. We create an array of constraint expressions using ```<=```, ```>=```, and ```==``` and allowed atoms and operations.\n",
    "\n",
    "```constraints = [expression1, expression2, expression3, expression4]```\n",
    "\n",
    "We can also add expressions to our constraint list using ```+=``` \n",
    "\n",
    "```constraints += expression5```\n",
    "\n",
    "in case the list is too big. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the first two constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [A*x <= b, x >= 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add three more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints += x <= 10\n",
    "constraints += x[2] <= 5\n",
    "constraints += x[1] + x[4] - x[2] <= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the problem. If the problem is unconstrained, we use\n",
    "\n",
    "```problem = minimize(objective)```\n",
    "\n",
    "If the problem is constrained, we use\n",
    "\n",
    "```problem = minimize(objective, constraints)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = minimize(objective, constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the problem with our ```SCS``` solver using ```solve!(problem, SCSSolver())```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve!(problem, SCSSolver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the optimizing value for a variable ```x``` after solving the problem using ```x.value``` and we obtain the optimal value of the objective function using ```problem.optval```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = x.value # Should be all ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_star = problem.optval # Should be 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Solve Exercise 9.5 (An optimal breakfast) on page 342 of your textbook  using ```Convex``` as we just did. Store the answer for the optimal cost in a variable called ```p_star``` and the optimizing amount of servings of Corn, Milk, Bread in a vector called ```x_star```. Note that your amounts of servings need not be integers and that the constraint on the maximum number of servings is per each food type. The correct answer and an answer check cell which you should run have been provided. Note further that you can define three $1$-dimensional variables OR you can define one $3$-dimensional variable depending on whether you prefer to express things in terms of dot products or linear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_star_ref = 3.7412\n",
    "x_star_ref = [6.5882, 10, 5.0588]\n",
    "@assert norm(p_star_ref-p_star) < 10^-3 && norm(x_star_ref-x_star) < 10^-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following three questions:\n",
    "\n",
    "Let $x \\in R^N$, $b \\in R^M$ and $A \\in R^{M \\times N}$.\n",
    "\n",
    "Let $||v||_p$ denote the $p$-norm of a vector $v \\in R^N$ or $v \\in R^M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, $N = 7$, $M = 4$ and $A$, $b$ are constants defined in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0.947931  0.0639746  0.0377269  0.399518   0.01464    0.952785   0.80076; \n",
    " 0.773775  0.607076   0.412492   0.0761318  0.947795   0.0368772  0.232726;\n",
    " 0.388207  0.165969   0.936847   0.608555   0.172854   0.1524     0.778316;\n",
    " 0.931932  0.302759   0.516603   0.272542   0.0454645  0.392813   0.890168];\n",
    "b = 10*[0.036360155695131624,\n",
    " 0.6573670082214775,  \n",
    " 0.030290614404246075,\n",
    " 0.37546120019099893];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& p^* = \\underset{x}{\\text{min}} & & ||Ax - b||_2^2\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Solve this convex optimization problem using ```Convex``` and store the the optimizing value of $x$ in a variable called ```x_1```. Store the optimal value of the objective in a variable called ```p_1```.\n",
    "\n",
    "Recall that you must find the atoms needed to define your expressions in https://www.juliaopt.org/Convex.jl/dev/operations/ and might not be able to use the usual linear algebra syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert norm(p_1) < 10^-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& p^* = \\underset{x}{\\text{min}} & & ||Ax - b||_2^2 + ||x||_1\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Solve this convex optimization problem using ```Convex``` and store the the optimizing value of $x$ in a variable called ```x_2```. Store the optimal value of the objective in a variable called ```p_2```.\n",
    "\n",
    "Recall that you must find the atoms needed to define your expressions in https://www.juliaopt.org/Convex.jl/dev/operations/ and might not be able to use the usual linear algebra syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_2_ref = 12.373478296466208\n",
    "@assert norm(p_2_ref - p_2) < 10^-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "We introduce a new variable $z \\in R^N$ so that we are now optimizing with respect to both $x$ and $z$. \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& p^* = \\underset{x, z}{\\text{min}} & & ||Ax - b||_2^2 + \\sum_{i = 1}^N z_i\\\\\n",
    "& \\text{subject to} & & x \\leq z\\\\\n",
    "& & & x \\geq -z\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Solve this convex optimization problem using ```Convex``` and store the the optimizing value of $x$ in a variable called ```x_3```. Store the optimal value of the objective in a variable called ```p_3```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Examine the values of ```x_1```, ```p_1```, ```x_2```, ```p_2```, ```x_3```, ```p_3``` and answer the following. \n",
    "- How do ```x_1``` and ```x_2``` compare? What is the effect of the $1$-norm term?\n",
    "- How do ```x_2``` and ```x_3``` compare?  Why do you think this is the case and why might one pose the optimization problem in this second way? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers: (Your written answer goes here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Numerical Exercise 1, we had this rank 2 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = [1.0  1.0  0.0  0.0;\n",
    " -1.0  0.0  1.0  0.0;\n",
    "  1.0  1.0  0.0  0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we wanted to project this vector ```v``` onto the span of its columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [4,7,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did this via Gram-Schmidt and the projection of ```v``` onto the span of its columns turned out to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_hat_ref = [5.5, 7.0, 5.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that anything in the span of the columns of $V \\in R^{3\\times4}$ can be expressed as $Vb$ for some $b \\in R^4$. So, the projection of $v$ onto the span of the columns of $V$ is $\\hat{v}= Vb$ for some $b$. \n",
    "\n",
    "Since the columns of $b$ are not linearly independent, this $b$ is not unique, i.e., there is more than one possible choice of $b$. Though we won't really care about that here, they all yield the same $Vb$. \n",
    "\n",
    "Recall further that the projection of $v$ onto the span of the columns of $V$ is the vector $\\hat{v}$ in the span of the columns $V$ which is closest in Euclidean distance to $v$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Find the projection of ```v``` onto the span of the columns of ```V``` by solving the appropriate optimization problem using ```Convex``` and store the projection in a variable called ```v_hat```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert norm(v_hat_ref-v_hat) < 10^-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Slalom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will need to solve Exercise 9.2 (A slalom problem) on page 341 of your textbook. Start by reading the question once before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the table has been entered for you here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Float64[0,4,8,12,16,20,24]\n",
    "y = Float64[4,5,4,6,5,7,4]\n",
    "c = Float64[0,3,2,2,1,2,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a plot of the upper and lower ends of the gates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y + c/2, marker=true)\n",
    "plot!(x, y - c/2, marker=true)\n",
    "xlabel!(\"x\")\n",
    "ylabel!(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is then to find the shortest path which fits between the two curves above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the optimal trajectory be the set of points $(x_0, z_0),\\dots,(x_6, z_6)$.\n",
    "\n",
    "The $x_i$ are fixed and the only thing to optimize is the vertical coordinates $z_i,\\; i = 0,\\dots,6$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Solve for this optimizing vector $z \\in R^7$ using ```Convex``` and store its value in a variable called ```z_```. Store the optimal value of the distance (the minimum distance) in a variable called ```p_star```. \n",
    "\n",
    "The correct answer and an answer checking cell has been provided which you should run. \n",
    "\n",
    "Note that to construct a vector of ```Convex``` variables and constants, you must do it with ```;```, for example, ```v = [a1; a2; a3]```, so that it is explicitly designated as a column vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot your path ```z_``` so you can see whether it violates the constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y + c/2, marker=true)\n",
    "plot!(x, y - c/2, marker=true)\n",
    "plot!(x, z_, marker=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ref = [4, 4.375, 4.75, 5.125, 5.5, 6, 4]\n",
    "p_star_ref = 24.5734\n",
    "@assert norm(p_star_ref-p_star) < 10^-4 && norm(z_ref-z_) < 10^-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Support-vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I will first direct you to Wikipedia to read a short section on linear support-vector machines (SVMs). It's concise and has a nice diagram: https://en.wikipedia.org/wiki/Support-vector_machine#Linear_SVM. In particular, we'll be interested in the soft-margin SVM, but you'll need to read both the hard-margin and soft-margin part to understand.\n",
    "\n",
    "Next, scroll all the way to the end of this page: https://www.juliaopt.org/Convex.jl/dev/operations/ and read the little note on type promotions and the ```max(x, 0)``` atom. \n",
    "\n",
    "The code for doing this part of the exercise is actually available in the documentation so you will just need to understand it and modify it accordingly. Here it is: https://www.juliaopt.org/Convex.jl/dev/examples/general_examples/svm/. You should read this as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've done this, we can start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this part, we'll do something in two dimensions just so you can easily visualize what's going on and in the second part, we'll revisit our classifying wheat seeds problem from the previous exercise and do the exact same thing in seven dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating a dataset of 200 randomly generated points in $R^2$ centered around different places. We'll assign the points the two categories $1$ and $-1$. The 200 points are stored in a matrix called ```X``` whose columns are those points and the labels for those points $1$ and $-1$ are stored in ```Y```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = zeros(2,200)\n",
    "Y = zeros(200)\n",
    "\n",
    "# Start of generating data\n",
    "for i = 1:200\n",
    "    coin_flip = abs(rand(Int64)%2) # 50-50 chance of being 1 or 0\n",
    "    \n",
    "    if (coin_flip == 0) \n",
    "        X[:,i] = randn(2,1) .+ 2.5*[-1,1]\n",
    "        Y[i] = 1\n",
    "    else\n",
    "        X[:,i] = randn(2,1) .+ 0.5*[1,1]\n",
    "        Y[i] = -1\n",
    "    end\n",
    "end\n",
    "# Standardize the data points to have zero mean and unit standard deviation\n",
    "means = mean(X, dims=2)[:] # Means\n",
    "std_factor = [(1.0./std(X, dims= 2)[i])*(i == j) for i = 1:2, j = 1:2] # 1 / (standard deviations)\n",
    "X = X .- means # Subtract means \n",
    "X = std_factor*X # Standardize \n",
    "# End of generating data\n",
    "\n",
    "scatter(X[1,Y .== 1],X[2,Y .== 1])\n",
    "scatter!(X[1,Y .== -1],X[2,Y .== -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we can obtain the data points corresponding to a particular label by indexing the data points with the \n",
    "\n",
    "boolean vector ```Y .== 1``` or ```Y .== -1```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to get the data points with the ```-1``` label, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,Y .== -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this in doing this next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using ```Convex``` and referring to the example in the Julia documentation here: https://www.juliaopt.org/Convex.jl/dev/examples/general_examples/svm/, solve for the optimizing $w \\in R^2$ and $b \\in R$ which describe the line best separating the labelled data points we just created. You can use $C = 1$. \n",
    "\n",
    "Store the values of the optimizing vector $w$ and scalar $b$ in variables called ```w_``` and ```b_``` respectively. \n",
    "\n",
    "Note that we don't call our data points from each category $x_i$ and $y_i$ as in the documentation, our ```X``` are all the data points and our ```Y``` is the corresponding labels so don't get confused by this. Moreover, you don't need to do exactly as in the documentation creating a function, just understand and copy what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line in the $x$-$y$ plane is given by $w_1x + w_2y = b$ so we plot it and the data accordingly. If you did this correctly, the plotted line should look like it's separating the classes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X[1,Y .== 1],X[2,Y .== 1])\n",
    "scatter!(X[1,Y .== -1],X[2,Y .== -1])\n",
    "x = [i for i = -1.5:0.1:1.5]\n",
    "y = (-w_[1]/w_[2])*x .+ b_/w_[2]\n",
    "plot!(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the accuracy of the classification of the points, i.e., what proportion of the points above the line are actually labelled $1$. Checking whether a point is above or below the line amounts to checking the sign of $w^Tx-b$ which is what we do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sign.(w_'*X .- b_)[:]\n",
    "accuracy = sum(predictions .== Y)/length(predictions)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did this correctly, the accuracy should be in the high $90$s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert accuracy > 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is the exact same cell which loads the wheat seeds data from the last exercise except for one difference. Rather than the ```Y```s containing $1$, $2$, $3$ for the three categories of seeds, the ```Y```s contain $1$ if the seed is of type $3$ and $-1$ otherwise. \n",
    "\n",
    "Refer to the instructions of the previous exercise if you don't recall the meaning and context of this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = readdlm(\"seeds_dataset.txt\", '\\t', '\\n') # Load data\n",
    "X = zeros(7,210) # Data\n",
    "Y = zeros(1,210) # Labels\n",
    "X[1,:] = convert(Array{Float64,1}, Data[:,1]) # 7 different attributes\n",
    "X[2,:] = convert(Array{Float64,1}, Data[:,2]) \n",
    "X[3,:] = convert(Array{Float64,1}, Data[:,3]) \n",
    "X[4,:] = convert(Array{Float64,1}, Data[:,4]) \n",
    "X[5,:] = convert(Array{Float64,1}, Data[:,5]) \n",
    "X[6,:] = convert(Array{Float64,1}, Data[:,6]) \n",
    "X[7,:] = convert(Array{Float64,1}, Data[:,7]) \n",
    "Y = Data[:,8] # Labels\n",
    "Y = [Y[i] == 3 ? 1 : -1 for i in 1:210]\n",
    "I = randperm(210) # Random permutation of indices\n",
    "I_TRAINING = I[1:160] # Training subset\n",
    "I_TESTING = I[160:210] # Testing subset\n",
    "X_TRAINING = X[:,I_TRAINING]\n",
    "Y_TRAINING = Y[I_TRAINING]\n",
    "means = mean(X_TRAINING, dims=2)[:] # Means\n",
    "std_factor = [(1.0./std(X_TRAINING, dims=2)[i])*(i == j) for i = 1:7, j = 1:7] # 1 / (standard deviations)\n",
    "X_TRAINING = X_TRAINING .- means # Subtract means from training data\n",
    "X_TRAINING = std_factor*X_TRAINING # Standardize training data\n",
    "X_TESTING = X[:,I_TESTING] \n",
    "Y_TESTING = Y[I_TESTING]\n",
    "X_TESTING = X_TESTING .- means # Subtract means and standardize testing data\n",
    "X_TESTING = std_factor*X_TESTING;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now attempt to identify the type $3$ seeds in the training set by using a separating hyperplane for the seven-dimensional seed features directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using ```Convex``` and repeating the exact same thing you did in the previous question, solve for the optimizing $w \\in R^7$ and $b \\in R$ which describe the hyperplane best separating the seven-dimensional data points in ```X_TRAINING``` corresponding to the $1$ and $-1$ labels which are in ```Y_TRAINING```. You can use $C = 1$. \n",
    "\n",
    "Store the values of the optimizing vector $w$ and scalar $b$ in variables called ```w_``` and ```b_``` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check the accuracy of identification of the type $3$ seeds (which we assign the label $1$) by applying our predictor on the testing set. As with the previous question, this categorization is based on the sign of $w^Tx-b$. If you did the previous part correctly, this accuracy should be in the $90$s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sign.(w_'*X_TESTING.- b_)[:]\n",
    "accuracy = sum(predictions .== Y_TESTING)/length(predictions)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert accuracy > 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
